{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of the root node: 2\n"
     ]
    }
   ],
   "source": [
    "# class TreeNode:\n",
    "#     def __init__(self, data, attributes=None):\n",
    "#         self.value = data  # value of the node\n",
    "#         self.attributes = attributes if attributes is not None else {}  # attributes in dict\n",
    "#         self.children = []  # list of child nodes\n",
    "    \n",
    "#     def __str__(self):\n",
    "#         return str(self.value)\n",
    "\n",
    "#     def add_child(self, child_node):\n",
    "#         self.children.append(child_node)\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, label=None, attributes=None, children=None):\n",
    "        self.value = label  # value of the node\n",
    "        self.attributes = attributes if attributes is not None else {}  # attributes in dict\n",
    "        self.children = children or []  # dict of child nodes\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "def find_depth(node):\n",
    "    # Base case: If the node has no children, its depth is 0\n",
    "    if not node.children:\n",
    "        return 0\n",
    "    else:\n",
    "        # Find the depth of each child node recursively\n",
    "        child_depths = [find_depth(child) for child in node.children]\n",
    "        # Return the maximum depth among child nodes + 1 (for the current node)\n",
    "        return max(child_depths) + 1\n",
    "\n",
    "# Example usage:\n",
    "root = TreeNode(\"Root\")\n",
    "child1 = TreeNode(\"Child 1\")\n",
    "child2 = TreeNode(\"Child 2\")\n",
    "grandchild1 = TreeNode(\"Grandchild 1\")\n",
    "grandchild2 = TreeNode(\"Grandchild 2\")\n",
    "\n",
    "root.add_child(child1)\n",
    "root.add_child(child2)\n",
    "child1.add_child(grandchild1)\n",
    "child2.add_child(grandchild2)\n",
    "\n",
    "# Calculate the depth of the root node\n",
    "root_depth = find_depth(root)\n",
    "print(\"Depth of the root node:\", root_depth)  # Should print 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root\n"
     ]
    }
   ],
   "source": [
    "print(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "def count_elements(lst):\n",
    "    element_counts = {}\n",
    "    for element in lst:\n",
    "        if element in element_counts:\n",
    "            element_counts[element] += 1\n",
    "        else:\n",
    "            element_counts[element] = 1\n",
    "    return element_counts\n",
    "\n",
    "def find_most_common_label(labels):\n",
    "    label_counts = count_elements(labels)\n",
    "    most_common_label = max(label_counts, key=lambda k: label_counts[k])\n",
    "    return most_common_label\n",
    "\n",
    "# 示例的标签数据\n",
    "Label = [\"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\"]\n",
    "\n",
    "most_common_label = find_most_common_label(Label)\n",
    "print(most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def count_elements(lst):\n",
    "    element_counts = {}\n",
    "    for element in lst:\n",
    "        if element in element_counts:\n",
    "            element_counts[element] += 1\n",
    "        else:\n",
    "            element_counts[element] = 1\n",
    "    return element_counts\n",
    "\n",
    "#TODO: Check return a leaf node with the most common label\n",
    "def find_most_common_label(labels):\n",
    "    label_counts = count_elements(labels)\n",
    "    most_common_label = max(label_counts, key=lambda k: label_counts[k])\n",
    "    return most_common_label\n",
    "            \n",
    "def choose_best_attribute(S, Attributes, Label, Target_var, class_list, purity_measurement='IG'):\n",
    "    print(\"starting choosing best attribute...\")\n",
    "    print(\"Target_var: \", Target_var)\n",
    "    print(\"class_list: \", class_list)\n",
    "    if purity_measurement == 'IG':\n",
    "        print(\"starting use purity measurement as Information Gain\")\n",
    "        # Calculate information gain for all attributes and return the one with the maximum IG\n",
    "        best_attribute = find_most_informative_feature(S, Label, Target_var, class_list)\n",
    "    elif purity_measurement == 'majority':\n",
    "        best_attribute = max(Attributes, key=lambda attr: calculate_me_gain(S, attr, Label))\n",
    "    elif purity_measurement == 'gini':\n",
    "        best_attribute = max(Attributes, key=lambda attr: calculate_gini_gain(S, attr, Label))\n",
    "    else:\n",
    "        print(\"Invalid purity measurement input. Only 'IG', 'majority', and 'gini' are supported. Defaulting to Information Gain.\")\n",
    "        best_attribute = max(Attributes, key=lambda attr: calculate_information_gain(S, attr, Label))\n",
    "\n",
    "    return best_attribute\n",
    "\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Calculates the entropy of the given data set for the target attr\n",
    "import math\n",
    "\n",
    "\n",
    "def calculate_total_entropy(data, label, class_list):\n",
    "    df = pd.DataFrame(data)\n",
    "    total_row = df.shape[0] #the total size of the dataset\n",
    "    total_entr = 0\n",
    "    \n",
    "    for c in class_list: #for each class in the label\n",
    "        total_class_count = df[df[label] == c].shape[0] #number of the class\n",
    "        total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) #entropy of the class\n",
    "        total_entr += total_class_entr #adding the class entropy to the total entropy of the dataset\n",
    "    print(\"total_entropy:\", total_entr)\n",
    "    \n",
    "    return total_entr\n",
    "\n",
    "\n",
    "def calc_entropy(feature_value_data, label, class_list):\n",
    "    class_count = feature_value_data.shape[0]\n",
    "    entropy = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        label_class_count = feature_value_data[feature_value_data[label] == c].shape[0] #row count of class c \n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:\n",
    "            probability_class = label_class_count/class_count #probability of the class\n",
    "            entropy_class = - probability_class * np.log2(probability_class)  #entropy\n",
    "        entropy += entropy_class\n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def calc_info_gain(feature_name, train_data, label, class_list):\n",
    "    print(\"starting calculating {} information gain\".format(feature_name))\n",
    "    df = pd.DataFrame(train_data)\n",
    "    print(\"data df in cal info gain: \", df)\n",
    "    feature_value_list = df[feature_name].unique()\n",
    "    print(\"feature_value_list: \",feature_value_list )\n",
    "    # feature_value_list = train_data[feature_name].unique() #unqiue values of the feature\n",
    "    total_row = df.shape[0]\n",
    "    print(\"total_row: \", total_row)\n",
    "    feature_info = 0.0\n",
    "    \n",
    "    for feature_value in feature_value_list:\n",
    "        print(\"starting feature_value={} in the list\".format(feature_value))\n",
    "        feature_value_data = df[df[feature_name] == feature_value] #filtering rows with that feature_value\n",
    "        print(\"feature_value_data: \", feature_value_data)\n",
    "        feature_value_count = feature_value_data.shape[0]\n",
    "        feature_value_entropy = calc_entropy(feature_value_data, label, class_list) #calculcating entropy for the feature value\n",
    "        feature_value_probability = feature_value_count/total_row\n",
    "        feature_info += feature_value_probability * feature_value_entropy #calculating information of the feature value\n",
    "        \n",
    "    return calculate_total_entropy(train_data, label, class_list) - feature_info #calculating information gain by subtracting\n",
    "\n",
    "def find_most_informative_feature(train_data, label, Target_var, class_list):\n",
    "    print(\"Target_var:\", Target_var)\n",
    "    print(\"starting finding the most informative feature\")\n",
    "    print(\"training data:\",train_data )\n",
    "    df = pd.DataFrame(train_data)\n",
    "    print(df)\n",
    "    # Get the feature columns (all columns except the label colum\n",
    "    feature_columns = df.columns[df.columns != Target_var]\n",
    "    # Convert feature_columns to a list if needed\n",
    "    feature_list = feature_columns.tolist()\n",
    "    print(\"feature_list: \", feature_list)\n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "    print(\"Target_var:\", Target_var)\n",
    "    print(\"class_list: \", class_list)\n",
    "    \n",
    "    for feature in feature_list:  #for each feature in the dataset\n",
    "        print(\"starting calculating feaure gain\")\n",
    "        \n",
    "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
    "        if max_info_gain < feature_info_gain: #selecting feature name with highest information gain\n",
    "            max_info_gain = feature_info_gain\n",
    "            max_info_feature = feature\n",
    "        print(\"max info feature: \", max_info_feature)\n",
    "            \n",
    "    return max_info_feature\n",
    "\n",
    "# def calculate_information_gain(data, attribute, target_attribute):\n",
    "#     \"\"\"\n",
    "#     Calculate information gain for a given attribute in the dataset based on entropy.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data: A list of dictionaries, where each dictionary represents a data point.\n",
    "#     - attribute: The name of the attribute for which information gain is calculated (as a string).\n",
    "#     - target_attribute: The name of the target attribute (the attribute to predict) (as a string).\n",
    "\n",
    "#     Returns:\n",
    "#     - information_gain: The information gain of the attribute.\n",
    "#     \"\"\"\n",
    "#     # information_gain = 1\n",
    "#     print(\"starting calculate information gain\")\n",
    "#     total_entropy = calculate_entropy(data, attribute, target_attribute)\n",
    "\n",
    "#     print(\"total_entropy: \", total_entropy)\n",
    "    \n",
    "#     attribute_values = set(entry[attribute] for entry in data)\n",
    "#     weighted_entropy = 0.0\n",
    "    \n",
    "#     for value in attribute_values:\n",
    "#         subset = [entry for entry in data if entry[attribute] == value]\n",
    "#         subset_entropy = calculate_entropy(subset, target_attribute)  # Pass attributes and target_attribute\n",
    "#         weight = len(subset) / len(data)\n",
    "#         weighted_entropy += weight * subset_entropy\n",
    "\n",
    "    \n",
    "#     information_gain = total_entropy - weighted_entropy\n",
    "#     return information_gain\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_majority_error(data, attribute, target_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the Majority Error (ME) \n",
    "    for a given attribute in the dataset.\n",
    "    \"\"\"\n",
    "    total_count = len(data)\n",
    "    majority_error = 0.0\n",
    "\n",
    "    for value in set([entry[attribute] for entry in data]):\n",
    "        subset = [entry for entry in data if entry[attribute] == value]\n",
    "        subset_count = len(subset)\n",
    "        class_counts = {}\n",
    "\n",
    "        for entry in subset:\n",
    "            class_label = entry[target_attribute]\n",
    "            if class_label in class_counts:\n",
    "                class_counts[class_label] += 1\n",
    "            else:\n",
    "                class_counts[class_label] = 1\n",
    "\n",
    "        majority_class_count = max(class_counts.values())\n",
    "        majority_error += (subset_count / total_count) * (1.0 - (majority_class_count / subset_count))\n",
    "\n",
    "    return majority_error\n",
    "\n",
    "def calculate_me_gain(data, attribute, target_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the Gain using Majority Error (ME) \n",
    "    for a given attribute in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    me = calculate_majority_error(data, attribute, target_attribute)\n",
    "    total_count = len(data)\n",
    "    me_gain = 1.0 - me\n",
    "\n",
    "    return me_gain\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_gini_index(data, attribute, target_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the Gini Index \n",
    "    for a given attribute in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    total_count = len(data)\n",
    "    gini_index = 0.0\n",
    "\n",
    "    for value in set([entry[attribute] for entry in data]):\n",
    "        subset = [entry for entry in data if entry[attribute] == value]\n",
    "        subset_count = len(subset)\n",
    "        class_counts = {}\n",
    "\n",
    "        for entry in subset:\n",
    "            class_label = entry[target_attribute]\n",
    "            if class_label in class_counts:\n",
    "                class_counts[class_label] += 1\n",
    "            else:\n",
    "                class_counts[class_label] = 1\n",
    "\n",
    "        gini_value = 1.0\n",
    "        for class_count in class_counts.values():\n",
    "            probability = class_count / subset_count\n",
    "            gini_value -= probability ** 2\n",
    "\n",
    "        gini_index += (subset_count / total_count) * gini_value\n",
    "\n",
    "    return gini_index\n",
    "\n",
    "def calculate_gini_gain(data, attribute, target_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the Gain using Gini Index \n",
    "    for a given attribute in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    gini = calculate_gini_index(data, attribute, target_attribute)\n",
    "    total_count = len(data)\n",
    "    gini_gain = 1.0 - gini\n",
    "\n",
    "    return gini_gain\n",
    "\n",
    "def ID3(S, Attributes, Label,Target_var, purity_measurement=None):\n",
    "    #Default purity measurement for ID3\n",
    "    if not purity_measurement:\n",
    "        purity_measurement = 'IG'  # Default purity measurement\n",
    "    print('purity_measurement:', purity_measurement)\n",
    "\n",
    "    # Check if leaf mode with the same label\n",
    "    unique_labels = set(Label)\n",
    "    class_list = list(unique_labels)\n",
    "    print(\"unique_labels: \", unique_labels)\n",
    "    print(\"length of unique labels: \", len(unique_labels))\n",
    "    print(\"class_list: \", class_list)\n",
    "    if len(unique_labels) == 1:\n",
    "        print('unique labels == 1')\n",
    "        return TreeNode(label=unique_labels.pop())\n",
    "    # Check if attribute is empty\n",
    "    elif not Attributes:\n",
    "        most_common_label = find_most_common_label(Label)\n",
    "        print(\"attibute is empty, find most common label: \", most_common_label)\n",
    "        return TreeNode(label=most_common_label)\n",
    "    else:\n",
    "        # Create a Root Node for tree\n",
    "        root = TreeNode()\n",
    "        print(\"starting create root node\")\n",
    "        # Choose the best attribute A to split S\n",
    "        # Support the input purity measurement of IG, majorty and gini. Here we set default is IG.\n",
    "        best_attribute = choose_best_attribute(S, Attributes, Label, Target_var, class_list,purity_measurement)\n",
    "        print(\"best attribute: \", best_attribute)\n",
    "        root.attributes = best_attribute\n",
    "\n",
    "        # Remove the chosen attribute from the list of attributes\n",
    "        remaining_attributes = [attr for attr in Attributes if attr != best_attribute]\n",
    "\n",
    "        # Split S into subsets based on the values of the best attribute\n",
    "        attribute_values = set(example[best_attribute] for example in S)\n",
    "\n",
    "        # deal with the remaining attributes for subset Sv, according to A=V\n",
    "        for value in attribute_values:\n",
    "            Sv = [example for example in S if example[best_attribute] == value]\n",
    "            # If Sv is empty, add leaf node with the most common value of label in S\n",
    "            if not Sv:\n",
    "                most_common_label = find_most_common_label(Label)\n",
    "                root.children[value] = TreeNode(label=most_common_label)\n",
    "            else:\n",
    "                return ID3(Sv, remaining_attributes, Label, purity_measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_measurement: IG\n",
      "unique_labels:  {'-', '+'}\n",
      "length of unique labels:  2\n",
      "class_list:  ['-', '+']\n",
      "starting create root node\n",
      "starting choosing best attribute...\n",
      "Target_var:  Play\n",
      "class_list:  ['-', '+']\n",
      "starting use purity measurement as Information Gain\n",
      "Target_var: Play\n",
      "starting finding the most informative feature\n",
      "training data: [{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'Play': '-'}, {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'Play': '-'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'Play': '+'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': '+'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': '+'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': '-'}, {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': '+'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': '-'}, {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': '+'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': '+'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': '+'}, {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'Play': '-'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': '+'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': '-'}]\n",
      "     Outlook Temperature Humidity    Wind Play\n",
      "0      Sunny         Hot     High    Weak    -\n",
      "1      Sunny         Hot     High  Strong    -\n",
      "2   Overcast         Hot     High    Weak    +\n",
      "3       Rain        Mild     High    Weak    +\n",
      "4       Rain        Cool   Normal    Weak    +\n",
      "5       Rain        Cool   Normal  Strong    -\n",
      "6   Overcast        Cool   Normal  Strong    +\n",
      "7      Sunny        Mild     High    Weak    -\n",
      "8      Sunny        Cool   Normal    Weak    +\n",
      "9       Rain        Mild   Normal    Weak    +\n",
      "10     Sunny        Mild   Normal  Strong    +\n",
      "11  Overcast        Mild     High  Strong    -\n",
      "12  Overcast         Hot   Normal    Weak    +\n",
      "13      Rain        Mild     High    Weak    -\n",
      "feature_list:  ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
      "Target_var: Play\n",
      "class_list:  ['-', '+']\n",
      "starting calculating feaure gain\n",
      "starting calculating Outlook information gain\n",
      "data df in cal info gain:       Outlook Temperature Humidity    Wind Play\n",
      "0      Sunny         Hot     High    Weak    -\n",
      "1      Sunny         Hot     High  Strong    -\n",
      "2   Overcast         Hot     High    Weak    +\n",
      "3       Rain        Mild     High    Weak    +\n",
      "4       Rain        Cool   Normal    Weak    +\n",
      "5       Rain        Cool   Normal  Strong    -\n",
      "6   Overcast        Cool   Normal  Strong    +\n",
      "7      Sunny        Mild     High    Weak    -\n",
      "8      Sunny        Cool   Normal    Weak    +\n",
      "9       Rain        Mild   Normal    Weak    +\n",
      "10     Sunny        Mild   Normal  Strong    +\n",
      "11  Overcast        Mild     High  Strong    -\n",
      "12  Overcast         Hot   Normal    Weak    +\n",
      "13      Rain        Mild     High    Weak    -\n",
      "feature_value_list:  ['Sunny' 'Overcast' 'Rain']\n",
      "total_row:  14\n",
      "starting feature_value=Sunny in the list\n",
      "feature_value_data:     Outlook Temperature Humidity    Wind Play\n",
      "0    Sunny         Hot     High    Weak    -\n",
      "1    Sunny         Hot     High  Strong    -\n",
      "7    Sunny        Mild     High    Weak    -\n",
      "8    Sunny        Cool   Normal    Weak    +\n",
      "10   Sunny        Mild   Normal  Strong    +\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['-', '-', '+', '+', '+', '-', '+', '-', '+', '+', '+', '-', '+', '-'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-5a91ba2aff82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTarget_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'IG'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# You can change purity_measurement as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-2e286a64ce0a>\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, Label, Target_var, purity_measurement)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Choose the best attribute A to split S\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Support the input purity measurement of IG, majorty and gini. Here we set default is IG.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mbest_attribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_best_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTarget_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpurity_measurement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best attribute: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_attribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-2e286a64ce0a>\u001b[0m in \u001b[0;36mchoose_best_attribute\u001b[0;34m(S, Attributes, Label, Target_var, class_list, purity_measurement)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting use purity measurement as Information Gain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Calculate information gain for all attributes and return the one with the maximum IG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mbest_attribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_most_informative_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTarget_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpurity_measurement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'majority'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbest_attribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcalculate_me_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-2e286a64ce0a>\u001b[0m in \u001b[0;36mfind_most_informative_feature\u001b[0;34m(train_data, label, Target_var, class_list)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting calculating feaure gain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfeature_info_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_info_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_info_gain\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfeature_info_gain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#selecting feature name with highest information gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mmax_info_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_info_gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-2e286a64ce0a>\u001b[0m in \u001b[0;36mcalc_info_gain\u001b[0;34m(feature_name, train_data, label, class_list)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature_value_data: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_value_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mfeature_value_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_value_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mfeature_value_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_value_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#calculcating entropy for the feature value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mfeature_value_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_value_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mfeature_info\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfeature_value_probability\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfeature_value_entropy\u001b[0m \u001b[0;31m#calculating information of the feature value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-2e286a64ce0a>\u001b[0m in \u001b[0;36mcalc_entropy\u001b[0;34m(feature_value_data, label, class_list)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mlabel_class_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_value_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_value_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#row count of class c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mentropy_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel_class_count\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3767\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3769\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5876\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5878\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5933\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5934\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5935\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5937\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['-', '-', '+', '+', '+', '-', '+', '-', '+', '+', '+', '-', '+', '-'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Sample Usage\n",
    "S = [\n",
    "    {\"Outlook\": \"Sunny\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"-\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Wind\": \"Strong\", \"Play\": \"-\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Strong\", \"Play\": \"-\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Strong\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"-\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temperature\": \"Mild\", \"Humidity\": \"Normal\", \"Wind\": \"Strong\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Strong\", \"Play\": \"-\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temperature\": \"Hot\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"+\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"-\"},\n",
    "]\n",
    "\n",
    "Attributes = [\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\"]\n",
    "Label = [\"-\", \"-\", \"+\", \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"+\", \"-\", \"+\", \"-\"]\n",
    "Target_var = 'Play'\n",
    "\n",
    "\n",
    "root = ID3(S, Attributes, Label, Target_var, purity_measurement='IG')  # You can change purity_measurement as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
