{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Decision Tree\")\n",
    "# from TreeNode import TreeNode\n",
    "from DecisionTree import ID3\n",
    "from utils import predict, calculate_error_rate, preprocess_numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TreeNode:\n",
    "#     def __init__(self, label=None, attributes=None, children=None):\n",
    "#         self.label = label  # value of the node\n",
    "#         self.attributes = attributes\n",
    "#         if children is None:\n",
    "#             self.children = []\n",
    "#         else:\n",
    "#             self.children = children\n",
    "\n",
    "#     def __str__(self, level=0):\n",
    "#         prefix = \"  \" * level\n",
    "#         result = prefix + f\"Attribute: {self.attributes}, Label: {self.label}\\n\"\n",
    "#         for child in self.children:\n",
    "#             result += prefix + f\"Child:\\n\"\n",
    "#             result += child.__str__(level + 1)\n",
    "#         return result\n",
    "\n",
    "#     def add_child(self, child_node):\n",
    "#         self.children.append(child_node)\n",
    "\n",
    "#     def predict(self, instance):\n",
    "#         \"\"\"\n",
    "#         instance: a single row from the test dataset\n",
    "#         return: predicted label\n",
    "#         \"\"\"\n",
    "#         node = self\n",
    "#         while node.children:\n",
    "#             attribute_name = node.attributes\n",
    "#             attribute_value = instance.loc[attribute_name]  # Using .loc to get the value by column name\n",
    "#             print(f\"Checking attribute {attribute_name} with value {attribute_value}\")  \n",
    "#             matched_child = None\n",
    "#             for child in node.children:\n",
    "#                 if child.attributes == attribute_value:\n",
    "#                     matched_child = child\n",
    "#                     break\n",
    "            \n",
    "#             if matched_child:\n",
    "#                 print(f\"Found matching child with attribute {matched_child.attributes}\")\n",
    "#                 node = matched_child\n",
    "#             else:\n",
    "#                 print(f\"No child matches attribute {attribute_name} with value {attribute_value}\")  # Add this\n",
    "#                 return \"default_label_or_most_common_label\" \n",
    "\n",
    "#         return node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# class BaggedTrees:\n",
    "#     def __init__(self, n_trees=10, max_depth=float('inf')):\n",
    "#         self.n_trees = n_trees\n",
    "#         self.trees = []\n",
    "#         self.max_depth = max_depth\n",
    "\n",
    "#     def fit(self, data, attributes):\n",
    "#         for _ in range(self.n_trees):\n",
    "#             # Bootstrap sampling\n",
    "#             bootstrap_data = data.sample(n=len(data), replace=True)\n",
    "#             tree = ID3(bootstrap_data, attributes, self.max_depth)\n",
    "#             self.trees.append(tree)\n",
    "\n",
    "#     def predict(self, instance):\n",
    "#         predictions = [tree.predict(instance) for tree in self.trees]\n",
    "#         return max(set(predictions), key=predictions.count)\n",
    "\n",
    "#     def batch_predict(self, df):\n",
    "#         return [self.predict(row) for _, row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: Weather, Label: None\n",
      "Child:\n",
      "  Attribute: Sunny, Label: None\n",
      "  Child:\n",
      "    Attribute: Wind, Label: None\n",
      "    Child:\n",
      "      Attribute: Weak, Label: Yes\n",
      "    Child:\n",
      "      Attribute: Strong, Label: Yes\n",
      "Child:\n",
      "  Attribute: Overcast, Label: Yes\n",
      "Child:\n",
      "  Attribute: Rainy, Label: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建一个简单的数据集\n",
    "data_dict = {\n",
    "    'Weather': ['Sunny', 'Overcast', 'Rainy', 'Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes']\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "attributes = ['Weather', 'Wind']\n",
    "tree_root = ID3(data, attributes, float('inf'))\n",
    "print(tree_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, df_test):\n",
    "    \"\"\"\n",
    "    tree: The tree that has been build from ID3\n",
    "    df_test: test dataset, dataframe\n",
    "    return: a list of all prediction labels\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for index, row in df_test.iterrows():\n",
    "        node = tree\n",
    "        while node.children: \n",
    "            attribute_name = node.attributes \n",
    "            attribute_value = row[attribute_name] \n",
    "            matched_child = None\n",
    "            for child in node.children:\n",
    "                if child.attributes == attribute_value:  \n",
    "                    matched_child = child  \n",
    "                    break\n",
    "            if matched_child:\n",
    "                node = matched_child\n",
    "                for subnode in node.children:\n",
    "                    node = subnode\n",
    "            else:\n",
    "                break\n",
    "        predictions.append(node.label)  \n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = predict(tree_root, data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes']\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "true_labels = data.iloc[:, -1].tolist()\n",
    "print(true_labels)\n",
    "error_rate = calculate_error_rate(predictions, true_labels)\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {\n",
    "    'Weather': ['Sunny', 'Overcast', 'Rainy', 'Sunny'],\n",
    "    'Wind': ['Strong', 'Weak', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['Yes', 'Yes', 'Yes', 'No']  # These are just example labels, you can adjust them if needed\n",
    "}\n",
    "\n",
    "test_data = pd.DataFrame(test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaggedTrees:\n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, data, attributes):\n",
    "        for _ in range(self.n_trees):\n",
    "            # 1. Sample with replacement from data\n",
    "            bootstrap_sample = data.sample(n=len(data), replace=True)\n",
    "            print(bootstrap_sample)\n",
    "            \n",
    "            # 2. Train a decision tree on this sample\n",
    "            tree = ID3(bootstrap_sample, attributes, float('inf'))\n",
    "            # print(\"tree: \",tree)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, dataset):\n",
    "        all_predictions = []\n",
    "\n",
    "        # For each instance in the dataset\n",
    "        for _, instance in dataset.iterrows():\n",
    "            # Predict with each tree and vote\n",
    "            predictions = [tree.predict(instance) for tree in self.trees]\n",
    "            # Append the majority vote to all_predictions\n",
    "            all_predictions.append(max(set(predictions), key=predictions.count))\n",
    "\n",
    "        return all_predictions\n",
    "    # def predict(self, instance):\n",
    "    #     # Predict with each tree and vote\n",
    "    #     predictions = [tree.predict(instance) for tree in self.trees]\n",
    "    #     return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Using it:\n",
    "# bagged_model = BaggedTrees(n_trees=50)\n",
    "# bagged_model.fit(df, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Weather    Wind PlayTennis\n",
      "0     Sunny    Weak        Yes\n",
      "1  Overcast  Strong        Yes\n",
      "2     Rainy    Weak        Yes\n",
      "3     Sunny    Weak         No\n",
      "4     Sunny  Strong        Yes\n",
      "5  Overcast  Strong        Yes\n",
      "6     Rainy    Weak        Yes\n",
      "7     Rainy  Strong        Yes\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees:\n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, data, attributes):\n",
    "        for _ in range(self.n_trees):\n",
    "            # 1. Sample with replacement from data\n",
    "            bootstrap_sample = data.sample(n=len(data), replace=True)\n",
    "            \n",
    "            # 2. Train a decision tree on this sample\n",
    "            tree = ID3(bootstrap_sample, attributes, float('inf'))\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_tree(self, tree, instance):\n",
    "        node = tree\n",
    "        while node.children: \n",
    "            attribute_name = node.attributes \n",
    "            attribute_value = instance[attribute_name]\n",
    "            matched_child = None\n",
    "            for child in node.children:\n",
    "                if child.attributes == attribute_value:  \n",
    "                    matched_child = child  \n",
    "                    break\n",
    "            if matched_child:\n",
    "                node = matched_child\n",
    "                for subnode in node.children:\n",
    "                    node = subnode\n",
    "            else:\n",
    "                break\n",
    "        return node.label\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        all_predictions = []\n",
    "\n",
    "        # For each instance in the dataset\n",
    "        for _, instance in dataset.iterrows():\n",
    "            # Predict with each tree and vote\n",
    "            predictions = [self.predict_tree(tree, instance) for tree in self.trees]\n",
    "            # Append the majority vote to all_predictions\n",
    "            all_predictions.append(max(set(predictions), key=predictions.count))\n",
    "\n",
    "        return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged_model = BaggedTrees(n_trees=1)\n",
    "# attributes = ['Weather', 'Wind']\n",
    "bagged_model.fit(data, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes']\n",
      "['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes']\n",
      "Training Error Rate 0.125\n"
     ]
    }
   ],
   "source": [
    "#training error\n",
    "predictions = bagged_model.predict(data)\n",
    "print(predictions)\n",
    "true_labels = data.iloc[:, -1].tolist()\n",
    "print(true_labels)\n",
    "error_rate = calculate_error_rate(predictions, true_labels)\n",
    "print(f\"Training Error Rate {error_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_error_rate:  0.25\n"
     ]
    }
   ],
   "source": [
    "# test error\n",
    "predicted_labels = bagged_model.predict(test_data)\n",
    "true_labels = test_data.iloc[:, -1].tolist()\n",
    "test_error_rate = calculate_error_rate(predicted_labels, true_labels)\n",
    "print(\"test_error_rate: \", test_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          job  marital  education default  balance housing loan   \n",
      "0    1     services  married  secondary      no        0     yes   no  \\\n",
      "1    1  blue-collar   single  secondary      no        0     yes  yes   \n",
      "2    1   technician  married  secondary      no        1      no  yes   \n",
      "3    1       admin.  married   tertiary      no        0     yes   no   \n",
      "4    0   management   single   tertiary      no        1      no   no   \n",
      "\n",
      "    contact  day month  duration  campaign  pdays  previous poutcome    y  \n",
      "0   unknown    0   may         0         0      0         0  unknown   no  \n",
      "1  cellular    0   feb         1         0      0         0  unknown   no  \n",
      "2  cellular    1   aug         1         0      1         1  success  yes  \n",
      "3  cellular    0   jul         1         0      0         0  unknown   no  \n",
      "4  cellular    0   apr         0         0      0         0  unknown  yes  \n"
     ]
    }
   ],
   "source": [
    "# Test for real data set\n",
    "def preprocess_data(df):\n",
    "    # Convert continuous attributes to binary\n",
    "    for column in ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']:\n",
    "        median = df[column].median()\n",
    "        df[column] = df[column].apply(lambda x: 1 if x > median else 0)\n",
    "    \n",
    "    # Note: For columns with \"unknown\", we'll leave them as is. Pandas will treat them as a separate category.\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the training and test data\n",
    "test_file_path = \"Data/bank-4/test.csv\"\n",
    "train_file_path = \"Data/bank-4/train.csv\"\n",
    "column_names = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']\n",
    "df_bank_train = pd.read_csv(train_file_path, names=column_names)\n",
    "df_bank_test = pd.read_csv(test_file_path, names=column_names)\n",
    "bank_attributes = df_bank_train.columns.tolist()[:-1]\n",
    "\n",
    "# Apply preprocessing to train and test datasets\n",
    "train_data = preprocess_data(df_bank_train)\n",
    "test_data = preprocess_data(df_bank_test)\n",
    "attributes = bank_attributes\n",
    "# Preview the preprocessed train data\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged Trees with n= 1 Training Error Rate 0.0798\n",
      "Bagged Trees with n= 1 Testing Error Rate 0.1904\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 2 Training Error Rate 0.0628\n",
      "Bagged Trees with n= 2 Testing Error Rate 0.173\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 3 Training Error Rate 0.0462\n",
      "Bagged Trees with n= 3 Testing Error Rate 0.1668\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 4 Training Error Rate 0.0394\n",
      "Bagged Trees with n= 4 Testing Error Rate 0.1656\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 5 Training Error Rate 0.0362\n",
      "Bagged Trees with n= 5 Testing Error Rate 0.1754\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 6 Training Error Rate 0.03\n",
      "Bagged Trees with n= 6 Testing Error Rate 0.1568\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 7 Training Error Rate 0.0276\n",
      "Bagged Trees with n= 7 Testing Error Rate 0.1614\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 8 Training Error Rate 0.0294\n",
      "Bagged Trees with n= 8 Testing Error Rate 0.1518\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 9 Training Error Rate 0.025\n",
      "Bagged Trees with n= 9 Testing Error Rate 0.159\n",
      "------------------------------------------------\n",
      "Bagged Trees with n= 10 Training Error Rate 0.0208\n",
      "Bagged Trees with n= 10 Testing Error Rate 0.1612\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 11):  # Looping n from 1 to 10\n",
    "    bagged_model = BaggedTrees(n_trees=n)\n",
    "    bagged_model.fit(train_data, attributes)\n",
    "\n",
    "    # Training error\n",
    "    predictions = bagged_model.predict(train_data)\n",
    "    true_labels_train = train_data.iloc[:, -1].tolist()\n",
    "    error_rate_train = calculate_error_rate(predictions, true_labels_train)\n",
    "    print(\"Bagged Trees with n= {} Training Error Rate {}\".format(n, error_rate_train))\n",
    "\n",
    "    # Testing error\n",
    "    predictions = bagged_model.predict(test_data)\n",
    "    true_labels_test = test_data.iloc[:, -1].tolist()\n",
    "    error_rate_test = calculate_error_rate(predictions, true_labels_test)\n",
    "    print(\"Bagged Trees with n= {} Testing Error Rate {}\".format(n, error_rate_test))\n",
    "    print(\"------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-6d82e8906332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Looping n from 1 to 500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbagged_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggedTrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbagged_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Training error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-9e3313bde5c1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, attributes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# 2. Train a decision tree on this sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mchild_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mID3\u001b[0;34m(S, Attributes, max_depth, purity_measurement, root)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Choose the best attribute A to split S\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mbest_attribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mfind_best_attribute\u001b[0;34m(data, attributes, class_list, purity_measurement)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#for each feature in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mfeature_info_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_info_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurity_measurement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_info_gain\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfeature_info_gain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#selecting feature name with highest information gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mmax_info_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_info_gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/utah-machine-learning/Decision Tree/DecisionTree.py\u001b[0m in \u001b[0;36mcalculate_info_gain\u001b[0;34m(feature_name, data, class_list, purity_measurement)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_value_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mfeature_value_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeature_value\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#filtering rows with that feature_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mfeature_value_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_value_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpurity_measurement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'entropy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6089\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6091\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6092\u001b[0m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_errors = []\n",
    "testing_errors = []\n",
    "\n",
    "for n in range(1, 500):  # Looping n from 1 to 500\n",
    "    bagged_model = BaggedTrees(n_trees=n)\n",
    "    bagged_model.fit(train_data, attributes)\n",
    "\n",
    "    # Training error\n",
    "    predictions = bagged_model.predict(train_data)\n",
    "    true_labels_train = train_data.iloc[:, -1].tolist()\n",
    "    error_rate_train = calculate_error_rate(predictions, true_labels_train)\n",
    "    training_errors.append(error_rate_train)\n",
    "\n",
    "    # Testing error\n",
    "    predictions = bagged_model.predict(test_data)\n",
    "    true_labels_test = test_data.iloc[:, -1].tolist()\n",
    "    error_rate_test = calculate_error_rate(predictions, true_labels_test)\n",
    "    testing_errors.append(error_rate_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), training_errors, label='Training Error', marker='o')\n",
    "plt.plot(range(1, 11), testing_errors, label='Testing Error', marker='x')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Error Rates vs. Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias_variance(bagged_model, test_data, true_labels):\n",
    "    \"\"\"\n",
    "    Calculate bias and variance for the bagged model.\n",
    "\n",
    "    bagged_model: The trained BaggedTrees model.\n",
    "    test_data: The test dataset.\n",
    "    true_labels: The true labels of the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        bias: Calculated bias\n",
    "        variance: Calculated variance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize bias and variance to zero\n",
    "    bias = 0.0\n",
    "    variance = 0.0\n",
    "    \n",
    "    # For each label in the test data\n",
    "    for idx, true_label in enumerate(true_labels):\n",
    "        avg_prediction = 0\n",
    "        tree_predictions = []\n",
    "        \n",
    "        # Predict with each tree\n",
    "        for tree in bagged_model.trees:\n",
    "            single_prediction = predict(tree, test_data.iloc[[idx]])\n",
    "            tree_predictions.append(single_prediction[0])  # [0] to get the single prediction\n",
    "        \n",
    "        # Calculate average prediction for this instance\n",
    "        avg_prediction = tree_predictions.count('yes') / len(tree_predictions)\n",
    "        \n",
    "        # Convert the true label to binary format (-1 or 1)\n",
    "        y = 1 if true_label == 'yes' else -1\n",
    "        \n",
    "        # Update bias and variance\n",
    "        bias += np.power(y - avg_prediction, 2)\n",
    "        variance += np.var(np.where(np.array(tree_predictions) == 'yes', 1, -1))\n",
    "    \n",
    "    # Calculate average bias and variance\n",
    "    avg_bias = bias / len(test_data)\n",
    "    avg_variance = variance / len(test_data)\n",
    "    \n",
    "    return avg_bias, avg_variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume your BaggedTrees model is trained and stored in the variable \"bagged_model\"\n",
    "true_labels = test_data.iloc[:, -1].tolist()\n",
    "bias, variance = calculate_bias_variance(bagged_model, test_data, true_labels)\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Variance:\", variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_data(train_data, sample_size=1000):\n",
    "    \"\"\"Sample data uniformly without replacement.\"\"\"\n",
    "    return train_data.sample(n=sample_size, replace=False)\n",
    "\n",
    "# Initialize the variables\n",
    "n_iterations = 100\n",
    "n_trees = 500\n",
    "bagged_predictors = []\n",
    "single_trees = []\n",
    "\n",
    "# REPEAT for 100 times\n",
    "for _ in range(n_iterations):\n",
    "    # STEP 1: Sample 1000 examples uniformly without replacement\n",
    "    sampled_data = sample_data(train_data)\n",
    "    \n",
    "    # STEP 2: Run bagged trees learning algorithm based on the 1000 training examples and learn 500 trees\n",
    "    bagged_model = BaggedTrees(n_trees=n_trees)\n",
    "    bagged_model.fit(sampled_data, attributes)\n",
    "    \n",
    "    # Store the model and the first tree\n",
    "    bagged_predictors.append(bagged_model)\n",
    "    single_trees.append(bagged_model.trees[0])\n",
    "\n",
    "# Now, you have 100 bagged predictors and 100 single trees\n",
    "# Let's compute bias and variance for a test example\n",
    "all_single_tree_predictions = []\n",
    "\n",
    "for test_example in test_data.iterrows():\n",
    "    single_tree_predictions = [predict(tree, pd.DataFrame([test_example[1]])) for tree in single_trees]\n",
    "    all_single_tree_predictions.append(single_tree_predictions)\n",
    "\n",
    "# Convert the predictions to a binary format for easier calculations\n",
    "binary_predictions = [[1 if pred[0] == 'yes' else -1 for pred in predictions] for predictions in all_single_tree_predictions]\n",
    "true_labels_binary = [1 if label == 'yes' else -1 for label in test_data.iloc[:, -1].tolist()]\n",
    "\n",
    "biases = []\n",
    "variances = []\n",
    "\n",
    "for i, test_example_predictions in enumerate(binary_predictions):\n",
    "    # Calculate bias for this test example\n",
    "    avg_prediction = np.mean(test_example_predictions)\n",
    "    bias = (true_labels_binary[i] - avg_prediction) ** 2\n",
    "    biases.append(bias)\n",
    "    \n",
    "    # Calculate variance for this test example\n",
    "    variance = np.var(test_example_predictions)\n",
    "    variances.append(variance)\n",
    "\n",
    "# Compute the average bias and variance\n",
    "avg_bias = np.mean(biases)\n",
    "avg_variance = np.mean(variances)\n",
    "\n",
    "print(\"Average Bias:\", avg_bias)\n",
    "print(\"Average Variance:\", avg_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
