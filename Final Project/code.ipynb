{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Utilities import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw MAFLD data.\n",
    "data_dir = \"Data/NTCMRC_all.xlsx\"\n",
    "df = pd.read_excel(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df as df1\n",
    "df1 = df.copy()\n",
    "\n",
    "# Replace '\\\\N' with NaN\n",
    "df1 = df1.replace('\\\\N', np.nan)\n",
    "\n",
    "# Specify the columns to be converted to FLOAT\n",
    "columns_to_convert1 = ['BMI', 'Triglyceride_y', 'gamgt', 'waist_y', 'mst', 'egfrn', 'Estimated_GFR_x', 'Alb_Cre_ratio', 'HOMA_IR', 'HS_CRP', \\\n",
    "                       'LDL_C_direct', 'LDL_C_HDL_C', 'Adiponectin', 'Leptin', 'Uric_Acid','Insulin', 'ALT_GPT']\n",
    "\n",
    "# Specify the columns to be converted to INT\n",
    "columns_to_convert2 = ['脂肪肝 fatty Liver (0:正常  1:mild 2:moderate 3:severe)', 'smoke', 'smoke_q', \\\n",
    "                       'sex', 'w', 'coffee', 'betel']\n",
    "\n",
    "# Convert the specified columns to float and fill missing/unconvertible values with NaN\n",
    "for column in columns_to_convert1:\n",
    "    df1[column] = pd.to_numeric(df1[column], errors='coerce')\n",
    "\n",
    "# Convert the specified columns to int and fill missing/unconvertible values with NaN\n",
    "for column in columns_to_convert2:\n",
    "    df1[column] = pd.to_numeric(df1[column], errors='coerce').astype(pd.Int64Dtype())\n",
    "\n",
    "# Calculate FLI using the formula and defined as df2\n",
    "df2 = df1.copy()\n",
    "df2['FLI'] = (np.exp(0.953 * np.log(df2['Triglyceride_y']) + 0.139 * df2['BMI'] + 0.718 * np.log(df2['gamgt']) \\\n",
    "     + 0.053 * df2['waist_y'] - 15.745)) / (1 + np.exp(0.953 * np.log(df2['Triglyceride_y']) \\\n",
    "    + 0.139 * df2['BMI'] + 0.718 * np.log(df2['gamgt']) + 0.053 * df2['waist_y'] - 15.745)) * 100\n",
    "\n",
    "# Derive FL_echo based on ultrasound results column\n",
    "df2['FL_echo'] = df2['脂肪肝 fatty Liver (0:正常  1:mild 2:moderate 3:severe)']\n",
    "df2['FL_echo'] = df2['FL_echo'].replace('<NA>', np.nan)\n",
    "df2['fl_status'] = df2.apply(utils.derive_fl_status, axis=1)\n",
    "\n",
    "#Derive homa_ir_check, hs_crp_check, and mst_total to determine MAFLD risk factors\n",
    "df2['homa_ir_check'] = df2['HOMA_IR'].apply(lambda x: 1 if x >= 2.5 else 0)\n",
    "df2['hs_crp_check'] = df2['HS_CRP'].apply(lambda x: 1 if x > 2 else 0)\n",
    "df2['mst_total'] = df2[['w', 'hyper', 'HDL', 'fg', 'trig', 'homa_ir_check', 'hs_crp_check']].sum(axis=1)\n",
    "\n",
    "df3 = utils.derive_MAFLD_with_multi_label(utils.derive_MAFLD(df2))\n",
    "df4 = utils.derive_CKD(df3)\n",
    "\n",
    "columns_to_extract = ['HBsAg_x', 'Anti_HCV_x']\n",
    "df5 = df4.copy()\n",
    "for column in columns_to_extract:\n",
    "    new_column_name = column + '_num'\n",
    "    df5[new_column_name] = df5[column].apply(utils.extract_numeric_value)\n",
    "\n",
    "# Filtering - for patients with same CMRC)id and year_come has more than two records\n",
    "count_2_or_more_year_come = df5.groupby(['CMRC_id', 'year_come']).filter(lambda x: len(x) >= 2)\n",
    "unique_patients = count_2_or_more_year_come['CMRC_id'].nunique()\n",
    "patients_to_remove = df5.groupby(['CMRC_id', 'year_come']).filter(lambda x: len(x) >= 2)['CMRC_id'].unique()\n",
    "df5_filtered = df5[~df5['CMRC_id'].isin(patients_to_remove)]\n",
    "\n",
    "df6 = utils.sliding_window_multi_label_data(df5_filtered, input_window_size=1, target_window_size=1)\n",
    "\n",
    "# Filtering - filter df with first year patient with NON-MAFLD and target year(2nd year) MAFLD status valid\n",
    "filtered_df1 = df6[(df6['t1_MAFLD_0'] == 1) & (df6['t2_MAFLD_0'] != -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyj\\AppData\\Local\\Temp\\ipykernel_4068\\2510877545.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_0'] = df8['t2_MAFLD_0']\n",
      "C:\\Users\\zyj\\AppData\\Local\\Temp\\ipykernel_4068\\2510877545.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_Obesity'] = df8['t2_MAFLD_Obesity']\n",
      "C:\\Users\\zyj\\AppData\\Local\\Temp\\ipykernel_4068\\2510877545.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_MD'] = df8['t2_MAFLD_MD']\n",
      "C:\\Users\\zyj\\AppData\\Local\\Temp\\ipykernel_4068\\2510877545.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_Diabetes'] = df8['t2_MAFLD_Diabetes']\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection - manually selected by domain expert\n",
    "\n",
    "# Drop ID relevant cols in the dataset\n",
    "columns_to_drop = ['CMRC_id','t1_CMRC_id', 't1_sid','t1_P_Number']\n",
    "df8 = filtered_df1.drop(columns=columns_to_drop)\n",
    "\n",
    "#Select key columns for conventional machine learning models\n",
    "columns = [\"sex\", \"age\", \"waist_y\", \"Glucose_AC_y\", \"Triglyceride_y\", \"HDL_C_y\", \"AST_GOT\", \"ALT_GPT\", \\\n",
    "          \"gamgt\", \"Insulin\", \"T_Cholesterol\", \"LDL_C_direct\", \"VLDL_C\", \"Non_HDL_C\", \"T_CHOL_HDL_C\", \\\n",
    "          \"LDL_C_HDL_C\", \"HS_CRP\", \"Hb_A1c\", \"Uric_Acid\", \"HBsAg_x\", \"Anti_HCV_x\", \"HOMA_IR\", \"Adiponectin\", \\\n",
    "           \"Leptin\", \"TotalVitaminD\", \"smoke\", \"smoke_q\", \"coffee\", \"betel\", \"BMI\", \"DM_determine\", \"w\", \"hyper\", \\\n",
    "           \"fg\", \"HDL\", \"trig\", \"sarcf\", \"ms2\", \"MNA\", \"AUDIT\", \"HBV_\", \"HCV_\", \"MAFLD\", \"CKD\", \\\n",
    "           'HBsAg_x_num', 'Anti_HCV_x_num', \\\n",
    "           'MAFLD_0', 'MAFLD_Obesity', 'MAFLD_MD', 'MAFLD_Diabetes', \\\n",
    "           'year_come']\n",
    "# prefixes = [\"t1_\", \"t2_\"]\n",
    "prefixes = [\"t1_\"]\n",
    "renamed_columns = utils.add_prefix(columns, prefixes)\n",
    "\n",
    "df9 = df8[renamed_columns]\n",
    "# df9['t2_MAFLD'] = df8['t2_MAFLD']\n",
    "df9['t2_MAFLD_0'] = df8['t2_MAFLD_0']\n",
    "df9['t2_MAFLD_Obesity'] = df8['t2_MAFLD_Obesity']\n",
    "df9['t2_MAFLD_MD'] = df8['t2_MAFLD_MD']\n",
    "df9['t2_MAFLD_Diabetes'] = df8['t2_MAFLD_Diabetes']\n",
    "\n",
    "# drop these cols as those been derived for numeric cols, remain alias *_num, and MAFLD status for 1 year\n",
    "cols_to_drop_only_MAFLD = ['t1_HBsAg_x',  't1_Anti_HCV_x', 't1_MAFLD', 't1_MAFLD_0',\\\n",
    "                           't1_MAFLD_Obesity', 't1_MAFLD_MD', 't1_MAFLD_Diabetes', \\\n",
    "                           ]\n",
    "\n",
    "df9_processed = df9.drop(cols_to_drop_only_MAFLD, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set split for train-validation-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前4年 training， 4-5 testing\n",
    "# filter for for training set\n",
    "filtered_df_train = df9_processed[df9_processed['t1_year_come'] <= 3]\n",
    "\n",
    "# filter for year4 for test set\n",
    "filtered_df_test = df9_processed[df9_processed['t1_year_come'] >= 4]\n",
    "\n",
    "# df9_processed.drop('t1_MAFLD_0', axis=1, inplace=True)\n",
    "features = df9_processed.columns.drop(['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD'])\n",
    "\n",
    "categorical_features = ['t1_sex', 't1_w', 't1_smoke', 't1_smoke_q', 't1_coffee', 't1_betel', 't1_DM_determine', 't1_CKD']\n",
    "numeric_features = df9_processed.columns.drop(categorical_features).drop(['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD'])\n",
    "X_categorical = df9_processed[categorical_features]\n",
    "X_numeric = df9_processed[numeric_features]\n",
    "\n",
    "# multi-label for target variable\n",
    "y = df9_processed[['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD']]\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str, drop_first=True)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is (8671, 48)\n",
      "The shape of the validation set is (500, 48)\n",
      "The shape of the testing set is (2027, 48)\n"
     ]
    }
   ],
   "source": [
    "# 前4年 training， 4-5 testing\n",
    "# filter for for training set\n",
    "df_train = df9_processed[df9_processed['t1_year_come'] <= 3]\n",
    "\n",
    "# filter for year4 for test set\n",
    "df_test_and_validation = df9_processed[df9_processed['t1_year_come'] >= 4]\n",
    "\n",
    "# Separate data with Train, validate, test set\n",
    "df_validation = df_test_and_validation.sample(n=500, random_state=1)\n",
    "df_test = df_test_and_validation.drop(df_validation.index)\n",
    "\n",
    "print(f\"The shape of the training set is {df_train.shape}\")\n",
    "print(f\"The shape of the validation set is {df_validation.shape}\")\n",
    "print(f\"The shape of the testing set is {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# feature processing for scaling and missing imputation\n",
    "X_train, y_train = utils.preprocess_features_and_target(df_train)\n",
    "X_val, y_val = utils.preprocess_features_and_target(df_validation)\n",
    "X_test, y_test = utils.preprocess_features_and_target(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all dataframes to float\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_val = X_val.astype(float)\n",
    "y_val = y_val.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target variables - aim to apply 4 classfier for each target label\n",
    "targets = ['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting Training Model with Logistic Regression---\n",
      "Best hyperparameters for t2_MAFLD_0: {'C': 0.1}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'C': 0.1}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'C': 1}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'C': 1}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.8776517020226936\n",
      "Hamming Loss: 0.06401085347804637\n",
      "F1 Score (Macro): 0.28828413284132837\n",
      "F1 Score (Micro): 0.8740596942489687\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.7967054077418115\n",
      "t2_MAFLD_Obesity: 0.8190598940944455\n",
      "t2_MAFLD_Diabetes: 0.9156662203981301\n",
      "t2_MAFLD_MD: 0.7963409519815474\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store AUC scores for each target\n",
    "auc_scores = {}\n",
    "\n",
    "# Initialize an empty DataFrame for predictions\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "print(\"---Starting Training Model with Logistic Regression---\")\n",
    "# Train a separate Logistic Regression model for each target\n",
    "for target in targets:\n",
    "    # Initialize Logistic Regression and GridSearchCV\n",
    "    lr = LogisticRegression(solver='liblinear', random_state=2023)\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_lr = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best hyperparameters for {target}: {best_params}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_prob = best_lr.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate AUC on the test set and store in the dictionary\n",
    "    auc_score = roc_auc_score(y_test[target], y_pred_prob)\n",
    "    auc_scores[target] = auc_score\n",
    "\n",
    "    # Add predictions to the DataFrame\n",
    "    y_pred_df[target] = best_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "accuracy = accuracy_score(y_test[targets], y_pred_df)\n",
    "print(f\"Overall Test Accuracy: {accuracy}\")\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "ham_loss = hamming_loss(y_test[targets], y_pred_array)\n",
    "print(f\"Hamming Loss: {ham_loss}\")\n",
    "f1_macro = f1_score(y_test[targets], y_pred_array, average='macro')\n",
    "f1_micro = f1_score(y_test[targets], y_pred_array, average='micro')\n",
    "print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, score in auc_scores.items():\n",
    "    print(f\"{target}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting Training Model with SVM---\n",
      "Best hyperparameters for t2_MAFLD_0: {'C': 10}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'C': 10}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'C': 10}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'C': 10}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.8584\n",
      "Hamming Loss: 0.0720\n",
      "F1 Score (Macro): 0.5500\n",
      "F1 Score (Micro): 0.9280\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.8065\n",
      "t2_MAFLD_Obesity: 0.8275\n",
      "t2_MAFLD_Diabetes: 0.9261\n",
      "t2_MAFLD_MD: 0.8243\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "\n",
    "print(\"---Starting Training Model with SVM---\")\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize SVC and GridSearchCV\n",
    "    svm = SVC(probability=True)\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "\n",
    "    # Store the predictions\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(y_test[targets], y_pred_array)\n",
    "ham_loss = hamming_loss(y_test[targets], y_pred_array)\n",
    "f1_scores_macro = [f1_score(y_test[target], y_pred_df[target], average='macro') for target in targets]\n",
    "f1_scores_micro = [f1_score(y_test[target], y_pred_df[target], average='micro') for target in targets]\n",
    "\n",
    "f1_macro_avg = np.mean(f1_scores_macro)\n",
    "f1_micro_avg = np.mean(f1_scores_micro)\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro_avg:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_avg:.4f}\")\n",
    "\n",
    "# AUC scores (assuming auc_scores dictionary is calculated earlier)\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting Training Model with SVM---\n",
      "Best hyperparameters for t2_MAFLD_0: {'C': 10}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'C': 10}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'C': 10}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'C': 10}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.9280\n",
      "Hamming Loss: 0.0720\n",
      "F1 Score (Macro): 0.9050\n",
      "F1 Score (Micro): 0.9280\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.7967\n",
      "t2_MAFLD_Obesity: 0.8191\n",
      "t2_MAFLD_Diabetes: 0.9157\n",
      "t2_MAFLD_MD: 0.7963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, hamming_loss, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "\n",
    "print(\"---Starting Training Model with SVM---\")\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize SVC and GridSearchCV\n",
    "    svm = SVC(probability=True)\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_svm.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    # print(\"AUC Scores for Each Target:\")\n",
    "    # auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    # print(f\"{target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "# # After evaluating all models\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "ham_loss = hamming_loss(true_labels_flat, pred_labels_flat)\n",
    "f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.9145\n",
      "Hamming Loss: 0.0855\n",
      "F1 Score (Macro): 0.8873\n",
      "F1 Score (Micro): 0.9145\n",
      "AUC (Micro): 0.9160\n",
      "AUC (Macro): 0.7038\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.8065\n",
      "t2_MAFLD_Obesity: 0.8275\n",
      "t2_MAFLD_Diabetes: 0.9261\n",
      "t2_MAFLD_MD: 0.8243\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'max_depth': [None, 10, 20, 30], \n",
    "              'min_samples_split': [2, 5, 10], \n",
    "              'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize DecisionTreeClassifier and GridSearchCV\n",
    "    dt_clf = DecisionTreeClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(dt_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_dt = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_dt.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_dt.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    # auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    # print(f\"Decision Tree AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "ham_loss = hamming_loss(true_labels_flat, pred_labels_flat)\n",
    "f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "print(f\"AUC (Micro): {auc_micro:.4f}\")\n",
    "print(f\"AUC (Macro): {auc_macro:.4f}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'learning_rate': 0.01, 'n_estimators': 200}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.9371\n",
      "Hamming Loss: 0.0629\n",
      "F1 Score (Macro): 0.9168\n",
      "F1 Score (Micro): 0.9371\n",
      "AUC (Micro): 0.9707\n",
      "AUC (Macro): 0.8347\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.7967\n",
      "t2_MAFLD_Obesity: 0.8191\n",
      "t2_MAFLD_Diabetes: 0.9157\n",
      "t2_MAFLD_MD: 0.7963\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize AdaBoost and GridSearchCV\n",
    "    ada_clf = AdaBoostClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(ada_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_ada = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_ada.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_ada.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    # print(f\"AdaBoost AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "ham_loss = hamming_loss(true_labels_flat, pred_labels_flat)\n",
    "f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "print(f\"AUC (Micro): {auc_micro:.4f}\")\n",
    "print(f\"AUC (Macro): {auc_macro:.4f}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.9364\n",
      "Hamming Loss: 0.0636\n",
      "F1 Score (Macro): 0.9159\n",
      "F1 Score (Micro): 0.9364\n",
      "AUC (Micro): 0.9669\n",
      "AUC (Macro): 0.7816\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.7967\n",
      "t2_MAFLD_Obesity: 0.8191\n",
      "t2_MAFLD_Diabetes: 0.9157\n",
      "t2_MAFLD_MD: 0.7963\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'n_estimators': [10, 50, 100], 'max_samples': [0.5, 1.0], 'max_features': [0.5, 1.0]}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize BaggingClassifier and GridSearchCV\n",
    "    bagging_clf = BaggingClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(bagging_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_bagging = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_bagging.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_bagging.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    # print(f\"Bagging AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "ham_loss = hamming_loss(true_labels_flat, pred_labels_flat)\n",
    "f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "print(f\"AUC (Micro): {auc_micro:.4f}\")\n",
    "print(f\"AUC (Macro): {auc_macro:.4f}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], \n",
    "    'max_depth': [10, 20, 30], \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize RandomForestClassifier and GridSearchCV\n",
    "    rf_clf = RandomForestClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    # print(f\"Random Forest AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "ham_loss = hamming_loss(true_labels_flat, pred_labels_flat)\n",
    "f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "print(f\"AUC (Micro): {auc_micro:.4f}\")\n",
    "print(f\"AUC (Macro): {auc_macro:.4f}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize GradientBoostingClassifier and GridSearchCV\n",
    "    gb_clf = GradientBoostingClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_gb = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_gb.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_gb.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    # print(f\"Gradient Boosting AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "# Convert DataFrame to numpy array for metric calculation\n",
    "y_pred_array = y_pred_df.to_numpy()\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "ham_loss = hamming_loss(true_labels_flat, pred_labels_flat)\n",
    "f1_macro = f1_score(true_labels_flat, pred_labels_flat, average='macro')\n",
    "f1_micro = f1_score(true_labels_flat, pred_labels_flat, average='micro')\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {ham_loss:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "print(f\"AUC (Micro): {auc_micro:.4f}\")\n",
    "print(f\"AUC (Macro): {auc_macro:.4f}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN - 1 hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN - FC\n",
    "# convert to PyTorch tensor\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train[targets].values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val[targets].values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test[targets].values, dtype=torch.float32)\n",
    "\n",
    "# create data loader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(val_loader, model, target_labels):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.data\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_predictions.append(predicted.cpu().numpy())\n",
    "\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    print(\"Labels shape:\", all_labels.shape, \"Predictions shape:\", all_predictions.shape)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions > 0.5)\n",
    "    f1_micro = f1_score(all_labels, all_predictions > 0.5, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_predictions > 0.5, average='macro')\n",
    "    hl = hamming_loss(all_labels, all_predictions > 0.5)\n",
    "\n",
    "    class_auc = {}\n",
    "    for i, label in enumerate(target_labels):\n",
    "        class_auc[f'{label}: AUC'] = roc_auc_score(all_labels[:, i], all_predictions[:, i])\n",
    "\n",
    "    auc_micro = roc_auc_score(all_labels, all_predictions, average='micro')\n",
    "    auc_macro = roc_auc_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    return accuracy, f1_micro, f1_macro, hl, auc_micro, auc_macro, class_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Adjusted to use hidden_size\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Adjusted to use hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Using sigmoid as the activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output size\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(targets)\n",
    "\n",
    "# Define criterion\n",
    "criterion = nn.BCELoss()  \n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'learning_rate': 0.01, 'hidden_size': 16}, Best Validation Loss: 0.19112213887274265\n"
     ]
    }
   ],
   "source": [
    "# Hyper param tuning in validation set\n",
    "hyperparam_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'hidden_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for lr in hyperparam_grid['learning_rate']:\n",
    "    for hidden_size in hyperparam_grid['hidden_size']:\n",
    "        model = SingleLayerNN(input_size, output_size, hidden_size)  # Adjust the model to take hidden_size\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for inputs, labels in val_loader:  # Assuming you have a val_loader\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "                val_loss /= len(val_loader)\n",
    "                \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_params = {'learning_rate': lr, 'hidden_size': hidden_size}\n",
    "\n",
    "print(f\"Best Params: {best_params}, Best Validation Loss: {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.16842494648881257\n",
      "Test Accuracy: 0.8830784410458806\n",
      "Hamming Loss: 0.06216082881105081\n",
      "F1 Score (Macro): 0.26622769614274466\n",
      "F1 Score (Micro): 0.8775510204081632\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.8065\n",
      "t2_MAFLD_Obesity: 0.8275\n",
      "t2_MAFLD_Diabetes: 0.9261\n",
      "t2_MAFLD_MD: 0.8243\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the model with the best parameters\n",
    "best_lr = best_params['learning_rate']\n",
    "best_hidden_size = best_params['hidden_size']\n",
    "best_model = SingleLayerNN(input_size, output_size, best_hidden_size)\n",
    "\n",
    "# Retrain the model on the full training dataset\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    best_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        y_pred.extend(outputs.detach().numpy())\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "# Convert predictions and true labels to binary format for evaluation\n",
    "y_pred_binary = np.round(y_pred)\n",
    "y_true_binary = np.round(y_true)\n",
    "\n",
    "# Calculate hamming loss\n",
    "ham_loss = hamming_loss(y_true_binary, y_pred_binary)\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_macro = f1_score(y_true_binary, y_pred_binary, average='macro')\n",
    "f1_micro = f1_score(y_true_binary, y_pred_binary, average='micro')\n",
    "\n",
    "accuracy = accuracy_score(np.round(y_true), np.round(y_pred))\n",
    "auc_scores = {target: roc_auc_score(y_true[:, i], y_pred[:, i]) for i, target in enumerate(targets)}\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Hamming Loss: {ham_loss}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro}\")\n",
    "print(f\"AUC Scores for Each Target:\")\n",
    "for target, auc in auc_scores.items():\n",
    "    print(f\"{target}: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Proposed - Multitask learning with Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEwithClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 4\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(51, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 4)\n",
    "        )\n",
    "\n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 4 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 51),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # classifier\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 4),\n",
    "            torch.nn.Sigmoid()  # multi label\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        classification = self.classifier(encoded)\n",
    "        return decoded, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_criterion = torch.nn.MSELoss()\n",
    "classification_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "def l1_regularization(model, lambda_param):\n",
    "    l1_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l1_loss += torch.norm(param, p=1)  \n",
    "    return lambda_param * l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lambda param = 0.01, L1 lambda = 0.001\n",
      "Validation Loss: 0.9365230798721313\n",
      "Training with lambda param = 0.01, L1 lambda = 0.01\n",
      "Validation Loss: 0.943535566329956\n",
      "Training with lambda param = 0.01, L1 lambda = 0.1\n",
      "Validation Loss: 1.2775330543518066\n",
      "Training with lambda param = 0.1, L1 lambda = 0.001\n",
      "Validation Loss: 1.0164237022399902\n",
      "Training with lambda param = 0.1, L1 lambda = 0.01\n",
      "Validation Loss: 1.037276268005371\n",
      "Training with lambda param = 0.1, L1 lambda = 0.1\n",
      "Validation Loss: 1.357501745223999\n",
      "Training with lambda param = 1, L1 lambda = 0.001\n",
      "Validation Loss: 1.5315965414047241\n",
      "Training with lambda param = 1, L1 lambda = 0.01\n",
      "Validation Loss: 1.8232976198196411\n",
      "Training with lambda param = 1, L1 lambda = 0.1\n",
      "Validation Loss: 2.1795289516448975\n",
      "Training with lambda param = 10, L1 lambda = 0.001\n",
      "Validation Loss: 6.1094818115234375\n",
      "Training with lambda param = 10, L1 lambda = 0.01\n",
      "Validation Loss: 7.371715545654297\n",
      "Training with lambda param = 10, L1 lambda = 0.1\n",
      "Validation Loss: 10.129236221313477\n"
     ]
    }
   ],
   "source": [
    "#Try Different Lambda\n",
    "lambda_params = [0.01, 0.1, 1, 10]\n",
    "l1_lambdas = [0.001, 0.01, 0.1]\n",
    "\n",
    "def train_model(lambda_param, l1_lambda, train_loader, val_loader):\n",
    "    model = AEwithClassifier()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            decoded_outputs, classification_outputs = model(inputs)\n",
    "            reconstruction_loss = reconstruction_criterion(decoded_outputs, inputs)\n",
    "            classification_loss = classification_criterion(classification_outputs, labels)\n",
    "            total_loss = lambda_param * reconstruction_loss + classification_loss + l1_regularization(model, l1_lambda)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Calculate performance on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            decoded_outputs, classification_outputs = model(inputs)\n",
    "            reconstruction_loss = reconstruction_criterion(decoded_outputs, inputs)\n",
    "            classification_loss = classification_criterion(classification_outputs, labels)\n",
    "            val_loss += lambda_param * reconstruction_loss + classification_loss\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    return model, val_loss\n",
    "\n",
    "# Hyperparameter tuning\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for lambda_param in lambda_params:\n",
    "    for l1_lambda in l1_lambdas:\n",
    "        print(f\"Training with lambda param = {lambda_param}, L1 lambda = {l1_lambda}\")\n",
    "        model, val_loss = train_model(lambda_param, l1_lambda, train_loader, val_loader)\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_params = {'lambda_param': lambda_param, 'l1_lambda': l1_lambda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'lambda_param': 0.01, 'l1_lambda': 0.001}\n",
      "Accuracy: 88.55%\n",
      "F1 Score (Micro): 0.8756\n",
      "F1 Score (Macro): 0.2348\n",
      "Hamming Loss: 0.0629\n",
      "AUC (Micro): 0.9401\n",
      "AUC (Macro): 0.4270\n",
      "AUC for t2_MAFLD_0: 0.4641\n",
      "AUC for t2_MAFLD_Obesity: 0.4693\n",
      "AUC for t2_MAFLD_Diabetes: 0.3597\n",
      "AUC for t2_MAFLD_MD: 0.4149\n"
     ]
    }
   ],
   "source": [
    "# Retrain model with best parameters\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "best_model = train_model(best_params['lambda_param'], best_params['l1_lambda'], train_loader, val_loader)[0]\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = utils.calculate_performance_AEclassifier(test_loader, best_model)  \n",
    "# print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6353",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
