{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Utilities import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw MAFLD data.\n",
    "data_dir = \"Data/NTCMRC_all.xlsx\"\n",
    "df = pd.read_excel(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df as df1\n",
    "df1 = df.copy()\n",
    "\n",
    "# Replace '\\\\N' with NaN\n",
    "df1 = df1.replace('\\\\N', np.nan)\n",
    "\n",
    "# Specify the columns to be converted to FLOAT\n",
    "columns_to_convert1 = ['BMI', 'Triglyceride_y', 'gamgt', 'waist_y', 'mst', 'egfrn', 'Estimated_GFR_x', 'Alb_Cre_ratio', 'HOMA_IR', 'HS_CRP', \\\n",
    "                       'LDL_C_direct', 'LDL_C_HDL_C', 'Adiponectin', 'Leptin', 'Uric_Acid','Insulin', 'ALT_GPT']\n",
    "\n",
    "# Specify the columns to be converted to INT\n",
    "columns_to_convert2 = ['脂肪肝 fatty Liver (0:正常  1:mild 2:moderate 3:severe)', 'smoke', 'smoke_q', \\\n",
    "                       'sex', 'w', 'coffee', 'betel']\n",
    "\n",
    "# Convert the specified columns to float and fill missing/unconvertible values with NaN\n",
    "for column in columns_to_convert1:\n",
    "    df1[column] = pd.to_numeric(df1[column], errors='coerce')\n",
    "\n",
    "# Convert the specified columns to int and fill missing/unconvertible values with NaN\n",
    "for column in columns_to_convert2:\n",
    "    df1[column] = pd.to_numeric(df1[column], errors='coerce').astype(pd.Int64Dtype())\n",
    "\n",
    "# Calculate FLI using the formula and defined as df2\n",
    "df2 = df1.copy()\n",
    "df2['FLI'] = (np.exp(0.953 * np.log(df2['Triglyceride_y']) + 0.139 * df2['BMI'] + 0.718 * np.log(df2['gamgt']) \\\n",
    "     + 0.053 * df2['waist_y'] - 15.745)) / (1 + np.exp(0.953 * np.log(df2['Triglyceride_y']) \\\n",
    "    + 0.139 * df2['BMI'] + 0.718 * np.log(df2['gamgt']) + 0.053 * df2['waist_y'] - 15.745)) * 100\n",
    "\n",
    "# Derive FL_echo based on ultrasound results column\n",
    "df2['FL_echo'] = df2['脂肪肝 fatty Liver (0:正常  1:mild 2:moderate 3:severe)']\n",
    "df2['FL_echo'] = df2['FL_echo'].replace('<NA>', np.nan)\n",
    "df2['fl_status'] = df2.apply(utils.derive_fl_status, axis=1)\n",
    "\n",
    "#Derive homa_ir_check, hs_crp_check, and mst_total to determine MAFLD risk factors\n",
    "df2['homa_ir_check'] = df2['HOMA_IR'].apply(lambda x: 1 if x >= 2.5 else 0)\n",
    "df2['hs_crp_check'] = df2['HS_CRP'].apply(lambda x: 1 if x > 2 else 0)\n",
    "df2['mst_total'] = df2[['w', 'hyper', 'HDL', 'fg', 'trig', 'homa_ir_check', 'hs_crp_check']].sum(axis=1)\n",
    "\n",
    "df3 = utils.derive_MAFLD_with_multi_label(utils.derive_MAFLD(df2))\n",
    "df4 = utils.derive_CKD(df3)\n",
    "\n",
    "columns_to_extract = ['HBsAg_x', 'Anti_HCV_x']\n",
    "df5 = df4.copy()\n",
    "for column in columns_to_extract:\n",
    "    new_column_name = column + '_num'\n",
    "    df5[new_column_name] = df5[column].apply(utils.extract_numeric_value)\n",
    "\n",
    "# Filtering - for patients with same CMRC)id and year_come has more than two records\n",
    "count_2_or_more_year_come = df5.groupby(['CMRC_id', 'year_come']).filter(lambda x: len(x) >= 2)\n",
    "unique_patients = count_2_or_more_year_come['CMRC_id'].nunique()\n",
    "patients_to_remove = df5.groupby(['CMRC_id', 'year_come']).filter(lambda x: len(x) >= 2)['CMRC_id'].unique()\n",
    "df5_filtered = df5[~df5['CMRC_id'].isin(patients_to_remove)]\n",
    "\n",
    "df6 = utils.sliding_window_multi_label_data(df5_filtered, input_window_size=1, target_window_size=1)\n",
    "\n",
    "# Filtering - filter df with first year patient with NON-MAFLD and target year(2nd year) MAFLD status valid\n",
    "filtered_df1 = df6[(df6['t1_MAFLD_0'] == 1) & (df6['t2_MAFLD_0'] != -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u1462352\\AppData\\Local\\Temp\\ipykernel_20936\\2510877545.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_0'] = df8['t2_MAFLD_0']\n",
      "C:\\Users\\u1462352\\AppData\\Local\\Temp\\ipykernel_20936\\2510877545.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_Obesity'] = df8['t2_MAFLD_Obesity']\n",
      "C:\\Users\\u1462352\\AppData\\Local\\Temp\\ipykernel_20936\\2510877545.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_MD'] = df8['t2_MAFLD_MD']\n",
      "C:\\Users\\u1462352\\AppData\\Local\\Temp\\ipykernel_20936\\2510877545.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t2_MAFLD_Diabetes'] = df8['t2_MAFLD_Diabetes']\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection - manually selected by domain expert\n",
    "\n",
    "# Drop ID relevant cols in the dataset\n",
    "columns_to_drop = ['CMRC_id','t1_CMRC_id', 't1_sid','t1_P_Number']\n",
    "df8 = filtered_df1.drop(columns=columns_to_drop)\n",
    "\n",
    "#Select key columns for conventional machine learning models\n",
    "columns = [\"sex\", \"age\", \"waist_y\", \"Glucose_AC_y\", \"Triglyceride_y\", \"HDL_C_y\", \"AST_GOT\", \"ALT_GPT\", \\\n",
    "          \"gamgt\", \"Insulin\", \"T_Cholesterol\", \"LDL_C_direct\", \"VLDL_C\", \"Non_HDL_C\", \"T_CHOL_HDL_C\", \\\n",
    "          \"LDL_C_HDL_C\", \"HS_CRP\", \"Hb_A1c\", \"Uric_Acid\", \"HBsAg_x\", \"Anti_HCV_x\", \"HOMA_IR\", \"Adiponectin\", \\\n",
    "           \"Leptin\", \"TotalVitaminD\", \"smoke\", \"smoke_q\", \"coffee\", \"betel\", \"BMI\", \"DM_determine\", \"w\", \"hyper\", \\\n",
    "           \"fg\", \"HDL\", \"trig\", \"sarcf\", \"ms2\", \"MNA\", \"AUDIT\", \"HBV_\", \"HCV_\", \"MAFLD\", \"CKD\", \\\n",
    "           'HBsAg_x_num', 'Anti_HCV_x_num', \\\n",
    "           'MAFLD_0', 'MAFLD_Obesity', 'MAFLD_MD', 'MAFLD_Diabetes', \\\n",
    "           'year_come']\n",
    "# prefixes = [\"t1_\", \"t2_\"]\n",
    "prefixes = [\"t1_\"]\n",
    "renamed_columns = utils.add_prefix(columns, prefixes)\n",
    "\n",
    "df9 = df8[renamed_columns]\n",
    "# df9['t2_MAFLD'] = df8['t2_MAFLD']\n",
    "df9['t2_MAFLD_0'] = df8['t2_MAFLD_0']\n",
    "df9['t2_MAFLD_Obesity'] = df8['t2_MAFLD_Obesity']\n",
    "df9['t2_MAFLD_MD'] = df8['t2_MAFLD_MD']\n",
    "df9['t2_MAFLD_Diabetes'] = df8['t2_MAFLD_Diabetes']\n",
    "\n",
    "# drop these cols as those been derived for numeric cols, remain alias *_num, and MAFLD status for 1 year\n",
    "cols_to_drop_only_MAFLD = ['t1_HBsAg_x',  't1_Anti_HCV_x', 't1_MAFLD', 't1_MAFLD_0',\\\n",
    "                           't1_MAFLD_Obesity', 't1_MAFLD_MD', 't1_MAFLD_Diabetes', \\\n",
    "                           ]\n",
    "\n",
    "df9_processed = df9.drop(cols_to_drop_only_MAFLD, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set split for train-validation-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前4年 training， 4-5 testing\n",
    "# filter for for training set\n",
    "filtered_df_train = df9_processed[df9_processed['t1_year_come'] <= 3]\n",
    "\n",
    "# filter for year4 for test set\n",
    "filtered_df_test = df9_processed[df9_processed['t1_year_come'] >= 4]\n",
    "\n",
    "# df9_processed.drop('t1_MAFLD_0', axis=1, inplace=True)\n",
    "features = df9_processed.columns.drop(['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD'])\n",
    "\n",
    "categorical_features = ['t1_sex', 't1_w', 't1_smoke', 't1_smoke_q', 't1_coffee', 't1_betel', 't1_DM_determine', 't1_CKD']\n",
    "numeric_features = df9_processed.columns.drop(categorical_features).drop(['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD'])\n",
    "X_categorical = df9_processed[categorical_features]\n",
    "X_numeric = df9_processed[numeric_features]\n",
    "\n",
    "# multi-label for target variable\n",
    "y = df9_processed[['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD']]\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str, drop_first=True)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is (8671, 48)\n",
      "The shape of the testing set is (2527, 48)\n"
     ]
    }
   ],
   "source": [
    "# 前4年 training， 4-5 testing\n",
    "# filter for for training set\n",
    "df_train = df9_processed[df9_processed['t1_year_come'] <= 3]\n",
    "\n",
    "# filter for year4 for test set\n",
    "df_test = df9_processed[df9_processed['t1_year_come'] >= 4]\n",
    "\n",
    "# Separate data with Train, validate, test set\n",
    "# df_validation = df_test_and_validation.sample(n=500, random_state=1)\n",
    "# df_test = df_test_and_validation.drop(df_validation.index)\n",
    "\n",
    "print(f\"The shape of the training set is {df_train.shape}\")\n",
    "# print(f\"The shape of the validation set is {df_validation.shape}\")\n",
    "print(f\"The shape of the testing set is {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# feature processing for scaling and missing imputation\n",
    "X_train, y_train = utils.preprocess_features_and_target(df_train)\n",
    "# X_val, y_val = utils.preprocess_features_and_target(df_validation)\n",
    "X_test, y_test = utils.preprocess_features_and_target(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target variables - aim to apply 4 classfier for each target label\n",
    "targets = ['t2_MAFLD_0', 't2_MAFLD_Obesity', 't2_MAFLD_Diabetes', 't2_MAFLD_MD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting Training Model with Logistic Regression---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'C': 0.1}\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'C': 0.1}\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'C': 1}\n",
      "Best hyperparameters for t2_MAFLD_MD: {'C': 1}\n",
      "\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.8737633557578156\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.795542583445592\n",
      "t2_MAFLD_Obesity: 0.8170088656249396\n",
      "t2_MAFLD_Diabetes: 0.9282339367634536\n",
      "t2_MAFLD_MD: 0.7903016380343588\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store AUC scores for each target\n",
    "auc_scores = {}\n",
    "\n",
    "# Initialize an empty DataFrame for predictions\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "print(\"---Starting Training Model with Logistic Regression---\")\n",
    "# Train a separate Logistic Regression model for each target\n",
    "for target in targets:\n",
    "    # Initialize Logistic Regression and GridSearchCV\n",
    "    lr = LogisticRegression(solver='liblinear', random_state=2023)\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_lr = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best hyperparameters for {target}: {best_params}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_prob = best_lr.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate AUC on the test set and store in the dictionary\n",
    "    auc_score = roc_auc_score(y_test[target], y_pred_prob)\n",
    "    auc_scores[target] = auc_score\n",
    "\n",
    "    # Add predictions to the DataFrame\n",
    "    y_pred_df[target] = best_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "accuracy = accuracy_score(y_test[targets], y_pred_df)\n",
    "print(f\"Overall Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Print AUC for each target\n",
    "print(\"AUC Scores for Each Target:\")\n",
    "for target, score in auc_scores.items():\n",
    "    print(f\"{target}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting Training Model with SVM---\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, hamming_loss, f1_score\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize DataFrames for storing probabilities and predictions\n",
    "# y_prob_df = pd.DataFrame()\n",
    "# y_pred_df = pd.DataFrame()\n",
    "\n",
    "# # Define the parameter grid for hyperparameter tuning\n",
    "# param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}\n",
    "\n",
    "# print(\"---Starting Training Model with SVM---\")\n",
    "# # Loop through each target variable for model training and evaluation\n",
    "# for target in targets:\n",
    "#     # Initialize SVC and GridSearchCV\n",
    "#     svm = SVC(probability=True)\n",
    "#     grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "#     # Perform hyperparameter tuning\n",
    "#     grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "#     # Best model and parameters after hyperparameter tuning\n",
    "#     best_svm = grid_search.best_estimator_\n",
    "#     print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "#     # Predict probabilities and classes on the test set\n",
    "#     print(\"\\n---Apply Best Hyper Param for Test Set---\")\n",
    "#     y_prob = best_svm.predict_proba(X_test)[:, 1]\n",
    "#     y_pred = best_svm.predict(X_test)\n",
    "\n",
    "#     # Store the probabilities and predictions\n",
    "#     y_prob_df[target] = y_prob\n",
    "#     y_pred_df[target] = y_pred\n",
    "\n",
    "#     # Calculate and print AUC for the target\n",
    "#     print(\"AUC Scores for Each Target:\")\n",
    "#     auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "#     print(f\"{target}: {auc_score:.4f}\")\n",
    "\n",
    "# # Calculate overall performance metrics\n",
    "# true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "# pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "# pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "# accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "\n",
    "# # Print overall results\n",
    "# print(f\"Overall Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Decision Tree AUC for t2_MAFLD_0: 0.6680\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Decision Tree AUC for t2_MAFLD_Obesity: 0.6740\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Decision Tree AUC for t2_MAFLD_Diabetes: 0.7176\n",
      "Best hyperparameters for t2_MAFLD_MD: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Decision Tree AUC for t2_MAFLD_MD: 0.7355\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.8766\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.6680\n",
      "t2_MAFLD_Obesity: 0.6740\n",
      "t2_MAFLD_Diabetes: 0.7176\n",
      "t2_MAFLD_MD: 0.7355\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'max_depth': [None, 10, 20, 30], \n",
    "              'min_samples_split': [2, 5, 10], \n",
    "              'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize DecisionTreeClassifier and GridSearchCV\n",
    "    dt_clf = DecisionTreeClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(dt_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_dt = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_dt.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_dt.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    print(f\"Decision Tree AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Scores for Each Target:\")\n",
    "for target in targets:\n",
    "    print(f\"{target}: {roc_auc_score(y_test[target], y_prob_df[target]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "AdaBoost AUC for t2_MAFLD_0: 0.7754\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "AdaBoost AUC for t2_MAFLD_Obesity: 0.8123\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "AdaBoost AUC for t2_MAFLD_Diabetes: 0.9330\n",
      "Best hyperparameters for t2_MAFLD_MD: {'learning_rate': 0.01, 'n_estimators': 200}\n",
      "AdaBoost AUC for t2_MAFLD_MD: 0.8425\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.9348\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.7754\n",
      "t2_MAFLD_Obesity: 0.8123\n",
      "t2_MAFLD_Diabetes: 0.9330\n",
      "t2_MAFLD_MD: 0.8425\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize AdaBoost and GridSearchCV\n",
    "    ada_clf = AdaBoostClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(ada_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_ada = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_ada.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_ada.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    print(f\"AdaBoost AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Scores for Each Target:\")\n",
    "for target in targets:\n",
    "    print(f\"{target}: {roc_auc_score(y_test[target], y_prob_df[target]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for t2_MAFLD_0: {'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
      "Bagging AUC for t2_MAFLD_0: 0.7270\n",
      "Best hyperparameters for t2_MAFLD_Obesity: {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
      "Bagging AUC for t2_MAFLD_Obesity: 0.7528\n",
      "Best hyperparameters for t2_MAFLD_Diabetes: {'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Bagging AUC for t2_MAFLD_Diabetes: 0.9173\n",
      "Best hyperparameters for t2_MAFLD_MD: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Bagging AUC for t2_MAFLD_MD: 0.7829\n",
      "---Apply Best Hyper Param for Test Set---\n",
      "Overall Test Accuracy: 0.9337\n",
      "AUC Scores for Each Target:\n",
      "t2_MAFLD_0: 0.7270\n",
      "t2_MAFLD_Obesity: 0.7528\n",
      "t2_MAFLD_Diabetes: 0.9173\n",
      "t2_MAFLD_MD: 0.7829\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'n_estimators': [10, 50, 100], 'max_samples': [0.5, 1.0], 'max_features': [0.5, 1.0]}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize BaggingClassifier and GridSearchCV\n",
    "    bagging_clf = BaggingClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(bagging_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_bagging = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_bagging.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_bagging.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    print(f\"Bagging AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Scores for Each Target:\")\n",
    "for target in targets:\n",
    "    print(f\"{target}: {roc_auc_score(y_test[target], y_prob_df[target]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], \n",
    "    'max_depth': [None, 10, 20, 30], \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize RandomForestClassifier and GridSearchCV\n",
    "    rf_clf = RandomForestClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    print(f\"Random Forest AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Scores for Each Target:\")\n",
    "for target in targets:\n",
    "    print(f\"{target}: {roc_auc_score(y_test[target], y_prob_df[target]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emsemble method - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for storing probabilities and predictions\n",
    "y_prob_df = pd.DataFrame()\n",
    "y_pred_df = pd.DataFrame()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Loop through each target variable for model training and evaluation\n",
    "for target in targets:\n",
    "    # Initialize GradientBoostingClassifier and GridSearchCV\n",
    "    gb_clf = GradientBoostingClassifier(random_state=2023)\n",
    "    grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Best model and parameters after hyperparameter tuning\n",
    "    best_gb = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {target}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predict probabilities and classes on the test set\n",
    "    y_prob = best_gb.predict_proba(X_test)[:, 1]\n",
    "    y_pred = best_gb.predict(X_test)\n",
    "\n",
    "    # Store the probabilities and predictions\n",
    "    y_prob_df[target] = y_prob\n",
    "    y_pred_df[target] = y_pred\n",
    "\n",
    "    # Calculate and print AUC for the target\n",
    "    auc_score = roc_auc_score(y_test[target], y_prob)\n",
    "    print(f\"Gradient Boosting AUC for {target}: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "true_labels_flat = np.hstack([y_test[t] for t in targets])\n",
    "pred_probs_flat = np.hstack([y_prob_df[t] for t in targets])\n",
    "pred_labels_flat = np.hstack([y_pred_df[t] for t in targets])\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "auc_micro = roc_auc_score(true_labels_flat, pred_probs_flat)\n",
    "auc_macro = np.mean([roc_auc_score(y_test[t], y_prob_df[t]) for t in targets])\n",
    "\n",
    "# Print overall results\n",
    "print(\"---Apply Best Hyper Param for Test Set---\")\n",
    "print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Scores for Each Target:\")\n",
    "for target in targets:\n",
    "    print(f\"{target}: {roc_auc_score(y_test[target], y_prob_df[target]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN - 1 hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN - 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Proposed - Multitask learning with Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6353",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
